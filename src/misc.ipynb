{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'a', 'wonderful', '&', 'film.']\n",
      "What a wonderful & film.\n"
     ]
    }
   ],
   "source": [
    "text = \"What a wonderful & film.\"\n",
    "a = text.split()\n",
    "print(a)\n",
    "b = \" \".join(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sequence:  A Titan RTX has 24GB of VRAM\n",
      "Tokenized sequence:  ['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']\n",
      "Encoded sequence:  [101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102]\n",
      "Decoded sequence:  [CLS] A Titan RTX has 24GB of VRAM [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Transformer's tokenizer - input_ids\n",
    "sequence = \"A Titan RTX has 24GB of VRAM\"\n",
    "print(\"Original sequence: \",sequence)\n",
    "tokenized_sequence = tokenizer.tokenize(sequence)\n",
    "print(\"Tokenized sequence: \",tokenized_sequence)\n",
    "encodings = tokenizer(sequence)\n",
    "encoded_sequence = encodings['input_ids']\n",
    "print(\"Encoded sequence: \", encoded_sequence)\n",
    "decoded_encodings=tokenizer.decode(encoded_sequence)\n",
    "print(\"Decoded sequence: \", decoded_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102]\n",
      "[CLS] A Titan RTX has 24GB of VRAM [SEP]\n"
     ]
    }
   ],
   "source": [
    "encodings = tokenizer.encode_plus(\n",
    "    sequence,\n",
    "    add_special_tokens=True,\n",
    ")\n",
    "print(encodings[\"input_ids\"])\n",
    "print(tokenizer.decode(encodings[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence a:  This is a short sequence.\n",
      "Sequence b:  This is a rather long sequence. It is at least longer than the sequence A.\n",
      "A's encoding length=8. \n",
      "B's encoding length=19\n",
      "Padded sequence(A,B): [[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]]\n",
      "Attention mask(A,B): [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Transformer's tokenizer - attention_mask\n",
    "sequence_a = \"This is a short sequence.\"\n",
    "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
    "print(\"Sequence a: \",sequence_a)\n",
    "print(\"Sequence b: \",sequence_b)\n",
    "encoded_sequence_a = tokenizer(sequence_a)[\"input_ids\"]\n",
    "encoded_sequence_b = tokenizer(sequence_b)[\"input_ids\"]\n",
    "print(\"A's encoding length={}. \\nB's encoding length={}\".format(len(encoded_sequence_a),len(encoded_sequence_b)))\n",
    "padded_sequence_ab = tokenizer([sequence_a,sequence_b],padding=True)\n",
    "print(\"Padded sequence(A,B):\", padded_sequence_ab[\"input_ids\"])\n",
    "print(\"Attention mask(A,B):\", padded_sequence_ab[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sequence(AB): [101, 1188, 1110, 170, 1603, 4954, 119, 102, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]\n",
      "Decoded sequence(AB): [CLS] This is a short sequence. [SEP] This is a rather long sequence. It is at least longer than the sequence A. [SEP]\n",
      "Token type ids(AB): [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Transformer's tokenizer - token type id\n",
    "encodings_ab = tokenizer(sequence_a, sequence_b)\n",
    "print(\"Encoded sequence(AB):\", encodings_ab[\"input_ids\"])\n",
    "decoded_ab = tokenizer.decode(encodings_ab[\"input_ids\"])\n",
    "print(\"Decoded sequence(AB):\", decoded_ab)\n",
    "print(\"Token type ids(AB):\", encodings_ab[\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(\"How old are you?\", \"I'm 6 years old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1731, 1385, 1132, 1128, 136, 102, 146, 112, 182, 127, 1201, 1385, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] How old are you? [SEP] I'm 6 years old [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(encoded_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102, 146, 112, 182, 170, 5650, 1115, 2947, 1114, 1103, 1148, 5650, 102], [101, 1262, 1330, 5650, 102, 1262, 146, 1431, 1129, 12544, 1114, 1103, 1248, 5650, 102], [101, 1262, 1103, 1304, 1304, 1314, 1141, 102, 1262, 146, 1301, 1114, 1103, 1304, 1314, 1141, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                   \"And another sentence\",\n",
    "                   \"And the very very last one\"]\n",
    "batch_of_second_sentences = [\"I'm a sentence that goes with the first sentence\",\n",
    "                             \"And I should be encoded with the second sentence\",\n",
    "                             \"And I go with the very last one\"]\n",
    "encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8667, 146, 112, 182, 170, 1423, 5650, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] Hello I'm a single sentence [SEP]\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer([\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"],is_split_into_words=True)\n",
    "print(encoded_input)\n",
    "print(tokenizer.decode(encoded_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102], [101, 1262, 1330, 5650, 102], [101, 1262, 1103, 1304, 1304, 1314, 1141, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "[CLS] And another sentence [SEP]\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [[\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"],\n",
    "                   [\"And\", \"another\", \"sentence\"],\n",
    "                   [\"And\", \"the\", \"very\", \"very\", \"last\", \"one\"]]\n",
    "encoded_inputs = tokenizer(batch_sentences, is_split_into_words=True)\n",
    "print(encoded_inputs)\n",
    "print(tokenizer.decode(encoded_inputs[\"input_ids\"][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102, 146, 112, 182, 170, 5650, 1115, 2947, 1114, 1103, 1148, 5650, 102], [101, 1262, 1330, 5650, 102, 1262, 146, 1431, 1129, 12544, 1114, 1103, 1248, 5650, 102], [101, 1262, 1103, 1304, 1304, 1314, 1141, 102, 1262, 146, 1301, 1114, 1103, 1304, 1314, 1141, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "[CLS] Hello I'm a single sentence [SEP] I'm a sentence that goes with the first sentence [SEP]\n"
     ]
    }
   ],
   "source": [
    "batch_of_second_sentences = [[\"I'm\", \"a\", \"sentence\", \"that\", \"goes\", \"with\", \"the\", \"first\", \"sentence\"],\n",
    "                             [\"And\", \"I\", \"should\", \"be\", \"encoded\", \"with\", \"the\", \"second\", \"sentence\"],\n",
    "                             [\"And\", \"I\", \"go\", \"with\", \"the\", \"very\", \"last\", \"one\"]]\n",
    "encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences, is_split_into_words=True)\n",
    "print(encoded_inputs)\n",
    "print(tokenizer.decode(encoded_inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: NEGATIVE, with score: 0.9991\n",
      "label: POSITIVE, with score: 0.9999\n"
     ]
    }
   ],
   "source": [
    "result = nlp(\"I hate you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
    "\n",
    "result = nlp(\"I love you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "Downloading: 100%|██████████| 1.76k/1.76k [00:00<00:00, 419kB/s]\n",
      "Downloading: 100%|██████████| 1.14G/1.14G [01:29<00:00, 13.7MB/s]\n",
      "Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 5.96kB/s]\n",
      "Downloading: 100%|██████████| 878k/878k [00:13<00:00, 68.6kB/s] \n",
      "Downloading: 100%|██████████| 446k/446k [00:05<00:00, 84.7kB/s] \n"
     ]
    }
   ],
   "source": [
    "nlp_summary =pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokens for phrase:  330\n",
      "Length of tokens for summary:  64\n"
     ]
    }
   ],
   "source": [
    "phrase = \"\"\"\n",
    "They manufactured machinery for sale and lease, ranging from commercial scales and industrial time recorders, meat and cheese slicers, to tabulators and punched cards. Thomas J. Watson, Sr., fired from the National Cash Register Company by John Henry Patterson, called on Flint and, in 1914, was offered a position at CTR.[12] Watson joined CTR as general manager then, 11 months later, was made President when court cases relating to his time at NCR were resolved.[13] Having learned Patterson's pioneering business practices, Watson proceeded to put the stamp of NCR onto CTR's companies.[14] He implemented sales conventions, \"generous sales incentives, a focus on customer service, an insistence on well-groomed, dark-suited salesmen and had an evangelical fervor for instilling company pride and loyalty in every worker\".[15][16] His favorite slogan, \"THINK\", became a mantra for each company's employees.[15] During Watson's first four years, revenues reached $9 million ($134 million today) and the company's operations expanded to Europe, South America, Asia and Australia.[15] Watson never liked the clumsy hyphenated name \"Computing-Tabulating-Recording Company\" and on February 14, 1924, chose to replace it with the more expansive title \"International Business Machines\" which had previously been used as the name of CTR's Canadian Division.[17] By 1933, most of the subsidiaries had been merged into one company, IBM\n",
    "\"\"\"\n",
    "tokens = tokenizer(phrase)\n",
    "print(\"Length of tokens for phrase: \", len(tokens[\"input_ids\"]))\n",
    "result = nlp_summary(phrase, max_length=63)[0]\n",
    "tokens = tokenizer(result['summary_text'])\n",
    "print(\"Length of tokens for summary: \", len(tokens[\"input_ids\"]))\n",
    "#print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Thomas J. Watson, Sr. was fired from the National Cash Register Company by John Henry Patterson in 1914 . Watson was made president of the company after court cases relating to his time at NCR were resolved . During Watson's first four years, revenues reached $9 million ($134 million today) and\n"
     ]
    }
   ],
   "source": [
    "print(result['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 5.69kB/s]\n",
      "Downloading: 100%|██████████| 433/433 [00:00<00:00, 65.4kB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:05<00:00, 42.0kB/s] \n",
      "Downloading: 100%|██████████| 426k/426k [00:04<00:00, 92.5kB/s] \n",
      "Downloading: 100%|██████████| 413M/413M [00:40<00:00, 10.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n",
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "\n",
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n",
      "Downloading: 100%|██████████| 473/473 [00:00<00:00, 71.7kB/s]\n",
      "Downloading: 100%|██████████| 249M/249M [00:21<00:00, 11.9MB/s] \n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 5.61kB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:03<00:00, 54.5kB/s] \n",
      "Downloading: 100%|██████████| 426k/426k [00:05<00:00, 78.8kB/s] \n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"question-answering\")\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'the task of extracting an answer from a text given a question', score: 0.6226, start: 34, end: 95\n",
      "Answer: 'SQuAD dataset', score: 0.5053, start: 147, end: 160\n"
     ]
    }
   ],
   "source": [
    "result = nlp(question=\"What is extractive question answering?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "\n",
    "result = nlp(question=\"What is a good example of a question answering dataset?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n",
      "Downloading: 100%|██████████| 998/998 [00:00<00:00, 257kB/s]\n",
      "Downloading: 100%|██████████| 1.24G/1.24G [02:06<00:00, 10.6MB/s]\n",
      "Downloading: 100%|██████████| 60.0/60.0 [00:00<00:00, 10.9kB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:02<00:00, 103kB/s]  \n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"ner\")\n",
    "\n",
    "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\\\n",
    "           close to the Manhattan Bridge which is visible from the window.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-ORG',\n",
       "  'score': 0.9995786,\n",
       "  'index': 1,\n",
       "  'word': 'Hu',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9909764,\n",
       "  'index': 2,\n",
       "  'word': '##gging',\n",
       "  'start': 2,\n",
       "  'end': 7},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9982225,\n",
       "  'index': 3,\n",
       "  'word': 'Face',\n",
       "  'start': 8,\n",
       "  'end': 12},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.999488,\n",
       "  'index': 4,\n",
       "  'word': 'Inc',\n",
       "  'start': 13,\n",
       "  'end': 16},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9994345,\n",
       "  'index': 11,\n",
       "  'word': 'New',\n",
       "  'start': 40,\n",
       "  'end': 43},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9993196,\n",
       "  'index': 12,\n",
       "  'word': 'York',\n",
       "  'start': 44,\n",
       "  'end': 48},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9993794,\n",
       "  'index': 13,\n",
       "  'word': 'City',\n",
       "  'start': 49,\n",
       "  'end': 53},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.98625815,\n",
       "  'index': 19,\n",
       "  'word': 'D',\n",
       "  'start': 79,\n",
       "  'end': 80},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9514269,\n",
       "  'index': 20,\n",
       "  'word': '##UM',\n",
       "  'start': 80,\n",
       "  'end': 82},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9336589,\n",
       "  'index': 21,\n",
       "  'word': '##BO',\n",
       "  'start': 82,\n",
       "  'end': 84},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.97616535,\n",
       "  'index': 28,\n",
       "  'word': 'Manhattan',\n",
       "  'start': 124,\n",
       "  'end': 133},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9914629,\n",
       "  'index': 29,\n",
       "  'word': 'Bridge',\n",
       "  'start': 134,\n",
       "  'end': 140}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(6)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(np.expand_dims(a, axis=1))\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(b.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation_en_to_de\")\n",
    "print(translator(\"Hugging Face is a technology company based in New York and Paris\", max_length=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e74620b5231c594759dd38196a43826cb876ef98d7d71c0d51134b02a08b650"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('p38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
